{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "import catboost as cb\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"train_data.pqt\")\n",
    "test_df = pd.read_parquet(\"test_df.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['start_cluster'] = test_df['start_cluster'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_first = test_df.groupby('id').first().reset_index()\n",
    "merged_df = pd.merge(test_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
    "\n",
    "grouped_df_second = test_df.groupby('id').nth(1).reset_index()\n",
    "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
    "\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = merged_df[merged_df['date'] == 'month_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_first = train_df.groupby('id').first().reset_index()\n",
    "merged_df = pd.merge(train_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
    "\n",
    "grouped_df_second = train_df.groupby('id').nth(1).reset_index()\n",
    "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
    "\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged_df[merged_df['date'] == 'month_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df.corr().abs()\n",
    "\n",
    "# Получение верхнего треугольника матрицы корреляции (без диагонали)\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Нахождение колонок, где корреляция больше 0.9\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "train_df =train_df.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =test_df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns = ['start_cluster', 'channel_code', 'city', 'city_type', \n",
    "                    'index_city_code', 'ogrn_month', 'ogrn_year', 'okved', 'segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clever_one_hot(df, col): #переписать в lable encoding?\n",
    "    top_4 = df[col].value_counts().index[:4]\n",
    "    df.loc[~df[col].isin(top_4), col] = 'other'\n",
    "    one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
    "    df = df.drop(col, axis=1)\n",
    "    return pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "def feature_prossesing(df):\n",
    "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
    "    df = df.drop('segment', axis=1)\n",
    "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
    "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
    "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
    "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
    "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
    "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
    "\n",
    "    del df['ogrn_year']\n",
    "    del df['ogrn_month']\n",
    "\n",
    "    for i in ['channel_code', 'city_type']:\n",
    "        df = clever_one_hot(df, i)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
    "    df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
    "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
    "    df = df.fillna(-1)\n",
    "\n",
    "    return df, category_mapping   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, category_mapping = feature_prossesing(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clever_one_hot(df, col, df1): #переписать в lable encoding?\n",
    "    top_4 = df1[col].value_counts().index[:4]\n",
    "    df.loc[~df[col].isin(top_4), col] = 'other'\n",
    "    one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
    "    df = df.drop(col, axis=1)\n",
    "    return pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "def feature_prossesing(df, df_1):\n",
    "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
    "    df = df.drop('segment', axis=1)\n",
    "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
    "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
    "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
    "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
    "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
    "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
    "\n",
    "    del df['ogrn_year']\n",
    "    del df['ogrn_month']\n",
    "\n",
    "    for i in ['channel_code', 'city_type']:\n",
    "        df = clever_one_hot(df, i, df_1)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
    "    #df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
    "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
    "    df = df.fillna(-1)\n",
    "\n",
    "    return df, category_mapping   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, category_mapping = feature_prossesing(test_df, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "табнет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['end_cluster']\n",
    "X = df.drop(['end_cluster'], axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy().squeeze()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy().squeeze()\n",
    "\n",
    "X_test  = test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = np.eye(17)[y_train]\n",
    "y_val_onehot = np.eye(17)[y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetMultiTaskClassifier(\n",
    "                                n_steps=17,\n",
    "                                # cat_idxs=cat_idxs,\n",
    "                                # cat_dims=cat_dims,\n",
    "                                # cat_emb_dim=1,\n",
    "                                optimizer_fn=torch.optim.Adam,\n",
    "                                optimizer_params=dict(lr=2e-2),\n",
    "                                scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n",
    "                                                  \"gamma\":0.9},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                                mask_type='entmax', # \"sparsemax\",\n",
    "                                lambda_sparse=0, # don't penalize for sparser attention\n",
    "                       \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train_onehot,\n",
    "    eval_set=[(X_val,y_val_onehot)],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50, # please be patient ^^\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=1,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict_proba(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
