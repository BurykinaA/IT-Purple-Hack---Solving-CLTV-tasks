{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b7bef09-9e86-4b19-bbb7-e0ccb7515328",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-14T22:50:38.627643Z",
          "iopub.status.busy": "2024-02-14T22:50:38.627255Z",
          "iopub.status.idle": "2024-02-14T22:50:39.496067Z",
          "shell.execute_reply": "2024-02-14T22:50:39.495215Z",
          "shell.execute_reply.started": "2024-02-14T22:50:38.627616Z"
        },
        "id": "8b7bef09-9e86-4b19-bbb7-e0ccb7515328",
        "tags": []
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0ddb399-18f4-4051-86c0-39f1c623eef5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-15T11:34:39.109032Z",
          "iopub.status.busy": "2024-02-15T11:34:39.108624Z",
          "iopub.status.idle": "2024-02-15T11:34:39.955360Z",
          "shell.execute_reply": "2024-02-15T11:34:39.954584Z",
          "shell.execute_reply.started": "2024-02-15T11:34:39.109007Z"
        },
        "id": "e0ddb399-18f4-4051-86c0-39f1c623eef5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import math\n",
        "import pickle\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import optuna\n",
        "import catboost as cb\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "WremRMspEMJA",
      "metadata": {
        "id": "WremRMspEMJA"
      },
      "outputs": [],
      "source": [
        "# !pip freeze | grep \"numpy\\|pandas\\|lightgbm\\|scikit-learn\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcf3aae-5417-42a3-9ed2-818b9aef0f2f",
      "metadata": {
        "id": "cbcf3aae-5417-42a3-9ed2-818b9aef0f2f"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "32c8b1b0-f95e-4cb5-bbc1-c0edb76568c1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-15T11:34:44.750723Z",
          "iopub.status.busy": "2024-02-15T11:34:44.750145Z",
          "iopub.status.idle": "2024-02-15T11:34:46.098228Z",
          "shell.execute_reply": "2024-02-15T11:34:46.097494Z",
          "shell.execute_reply.started": "2024-02-15T11:34:44.750694Z"
        },
        "id": "32c8b1b0-f95e-4cb5-bbc1-c0edb76568c1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"train_data.pqt\")\n",
        "test_df = pd.read_parquet(\"test_data.pqt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06044363",
      "metadata": {
        "id": "06044363"
      },
      "source": [
        "заполянем 5 месяцем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "70f9cc0b",
      "metadata": {
        "id": "70f9cc0b"
      },
      "outputs": [],
      "source": [
        "test_df['start_cluster'] = test_df['start_cluster'].fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d48a01",
      "metadata": {
        "id": "69d48a01"
      },
      "source": [
        "вытягиваем в колбасу по id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a38b5b6b",
      "metadata": {
        "id": "a38b5b6b"
      },
      "outputs": [],
      "source": [
        "grouped_df_first = test_df.groupby('id').first().reset_index()\n",
        "merged_df = pd.merge(test_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
        "\n",
        "grouped_df_second = test_df.groupby('id').nth(1).reset_index()\n",
        "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
        "\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45e22331",
      "metadata": {
        "id": "45e22331"
      },
      "outputs": [],
      "source": [
        "test_df = merged_df[merged_df['date'] == 'month_6']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "81f271d2",
      "metadata": {
        "id": "81f271d2"
      },
      "outputs": [],
      "source": [
        "grouped_df_first = train_df.groupby('id').first().reset_index()\n",
        "merged_df = pd.merge(train_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
        "\n",
        "grouped_df_second = train_df.groupby('id').nth(1).reset_index()\n",
        "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
        "\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "99ce934c",
      "metadata": {
        "id": "99ce934c"
      },
      "outputs": [],
      "source": [
        "train_df = merged_df[merged_df['date'] == 'month_3']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22b60ce",
      "metadata": {
        "id": "c22b60ce"
      },
      "source": [
        "удаляем сильную корреляцию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cbe87424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "cbe87424",
        "outputId": "9f282187-34c0-4fff-8f8b-33dffdc8fe94"
      },
      "outputs": [],
      "source": [
        "corr_matrix = train_df.corr().abs()\n",
        "\n",
        "# Получение верхнего треугольника матрицы корреляции (без диагонали)\n",
        "upper_triangle = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Нахождение колонок, где корреляция больше 0.9\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "\n",
        "train_df =train_df.drop(to_drop, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ec78c3b5",
      "metadata": {
        "id": "ec78c3b5"
      },
      "outputs": [],
      "source": [
        "test_df =test_df.drop(to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e50a2c22",
      "metadata": {
        "id": "e50a2c22"
      },
      "outputs": [],
      "source": [
        "category_columns = ['start_cluster', 'channel_code', 'city', 'city_type',\n",
        "                    'index_city_code', 'ogrn_month', 'ogrn_year', 'okved', 'segment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fdf0c11c",
      "metadata": {
        "id": "fdf0c11c"
      },
      "outputs": [],
      "source": [
        "# counts = train_df['city_type'].value_counts()\n",
        "\n",
        "# # Построение гистограммы\n",
        "# plt.bar(counts.index, counts.values)\n",
        "# plt.xlabel('Уникальные значения')\n",
        "# plt.ylabel('Частота')\n",
        "# plt.title('Гистограмма количества встречающихся раз категориальных значений')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dae8244",
      "metadata": {
        "id": "0dae8244"
      },
      "source": [
        "обработка текстов и нан"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a64e1d8c",
      "metadata": {
        "id": "a64e1d8c",
        "outputId": "503a5eba-5813-4e5d-8f18-db824a2383c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2          channel_code_5\n",
              "5          channel_code_2\n",
              "8         channel_code_12\n",
              "11        channel_code_14\n",
              "14         channel_code_8\n",
              "               ...       \n",
              "599987     channel_code_9\n",
              "599990    channel_code_14\n",
              "599993     channel_code_8\n",
              "599996     channel_code_9\n",
              "599999    channel_code_14\n",
              "Name: channel_code, Length: 200000, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['channel_code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c77de816",
      "metadata": {
        "id": "c77de816"
      },
      "outputs": [],
      "source": [
        "# def clever_one_hot(df, col): #переписать в lable encoding?\n",
        "#     top_4 = df[col].value_counts().index[:4]\n",
        "#     df.loc[~df[col].isin(top_4), col] = 'other'\n",
        "#     one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
        "#     df = df.drop(col, axis=1)\n",
        "#     return pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "def feature_prossesing(df):\n",
        "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
        "    df = df.drop('segment', axis=1)\n",
        "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
        "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
        "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
        "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
        "\n",
        "    del df['ogrn_year']\n",
        "    del df['ogrn_month']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    \n",
        "\n",
        "    for i in ['channel_code', 'city_type', 'okved']:\n",
        "        df[i] = label_encoder.fit_transform(df[i])\n",
        "\n",
        "    \n",
        "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
        "    df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
        "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    return df, category_mapping   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "571718d7",
      "metadata": {
        "id": "571718d7"
      },
      "outputs": [],
      "source": [
        "df, category_mapping = feature_prossesing(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "db0e4abe",
      "metadata": {
        "id": "db0e4abe"
      },
      "outputs": [],
      "source": [
        "def clever_one_hot(df, col, df1): #переписать в lable encoding?\n",
        "    top_4 = df1[col].value_counts().index[:4]\n",
        "    df.loc[~df[col].isin(top_4), col] = 'other'\n",
        "    one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
        "    df = df.drop(col, axis=1)\n",
        "    return pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "def feature_prossesing(df, df_1):\n",
        "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
        "    df = df.drop('segment', axis=1)\n",
        "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
        "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
        "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
        "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
        "\n",
        "    del df['ogrn_year']\n",
        "    del df['ogrn_month']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    \n",
        "\n",
        "    for i in ['channel_code', 'city_type', 'okved']:\n",
        "        df[i] = label_encoder.fit_transform(df[i])\n",
        "\n",
        "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
        "    #df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
        "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    return df, category_mapping   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fb386861",
      "metadata": {
        "id": "fb386861"
      },
      "outputs": [],
      "source": [
        "test, category_mapping = feature_prossesing(test_df, train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "78538828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78538828",
        "outputId": "864c1f22-c276-4c4b-9d5c-587fac9ff92c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'{other}': 0,\n",
              " '{}': 1,\n",
              " '{α, β}': 2,\n",
              " '{α, γ}': 3,\n",
              " '{α, δ}': 4,\n",
              " '{α, ε, η}': 5,\n",
              " '{α, ε, θ}': 6,\n",
              " '{α, ε, ψ}': 7,\n",
              " '{α, ε}': 8,\n",
              " '{α, η}': 9,\n",
              " '{α, θ}': 10,\n",
              " '{α, λ}': 11,\n",
              " '{α, μ}': 12,\n",
              " '{α, π}': 13,\n",
              " '{α, ψ}': 14,\n",
              " '{α}': 15,\n",
              " '{λ}': 16}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791378ad",
      "metadata": {
        "id": "791378ad"
      },
      "source": [
        "веса для рок аука которые идут в бустинг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c966fbb",
      "metadata": {
        "id": "1c966fbb"
      },
      "outputs": [],
      "source": [
        "def weighted_roc_auc(y_true, y_pred, labels, weights_dict):\n",
        "    unnorm_weights = np.array([weights_dict[label] for label in labels])\n",
        "    weights = unnorm_weights / unnorm_weights.sum()\n",
        "    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,\n",
        "                                    multi_class=\"ovr\", average=None)\n",
        "    return sum(weights * classes_roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "67791ff1",
      "metadata": {
        "id": "67791ff1"
      },
      "outputs": [],
      "source": [
        "cluster_weights = pd.read_excel(\"cluster_weights.xlsx\").set_index(\"cluster\")\n",
        "weights_dict = cluster_weights[\"unnorm_weight\"].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b6ea63c8",
      "metadata": {
        "id": "b6ea63c8"
      },
      "outputs": [],
      "source": [
        "weits = [0]*17\n",
        "\n",
        "for k, v in category_mapping.items():\n",
        "    weits[category_mapping[k]] = weights_dict[k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ec1aaf24",
      "metadata": {
        "id": "ec1aaf24",
        "outputId": "6e303d60-acdd-4660-b03a-f2d50e514b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 1, 3, 3, 3, 1, 1, 3, 2, 2, 1, 3, 2, 1, 3, 2, 2]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91feaa10",
      "metadata": {
        "id": "91feaa10"
      },
      "source": [
        "треним"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a81d6e35",
      "metadata": {
        "id": "a81d6e35"
      },
      "outputs": [],
      "source": [
        "y = df['end_cluster']\n",
        "X = df.drop(['end_cluster'], axis=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "183b2c43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install lightautoml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c8252ef0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# LightAutoML presets, task and report generation\n",
        "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.report.report_deco import ReportDeco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a5688eda",
      "metadata": {},
      "outputs": [],
      "source": [
        "task = Task('multiclass')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "5a796892",
      "metadata": {},
      "outputs": [],
      "source": [
        "automl = TabularUtilizedAutoML(\n",
        "    task = Task('multiclass'),\n",
        "    timeout = 3600,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "2d3e0acd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[02:43:03] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
            "[02:43:03] - time: 3600.00 seconds\n",
            "[02:43:03] - CPU: 4 cores\n",
            "[02:43:03] - memory: 16 GB\n",
            "\n",
            "[02:43:03] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
            "\n",
            "[02:43:03] ==================================================\n",
            "[02:43:03] Start 0 automl preset configuration:\n",
            "[02:43:03] \u001b[1md:\\Anaconda\\lib\\site-packages\\lightautoml\\automl\\presets\\tabular_configs\\conf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
            "[02:43:03] Stdout logging level is INFO.\n",
            "[02:43:03] Task: multiclass\n",
            "\n",
            "[02:43:03] Start automl preset with listed constraints:\n",
            "[02:43:03] - time: 3600.00 seconds\n",
            "[02:43:03] - CPU: 4 cores\n",
            "[02:43:03] - memory: 16 GB\n",
            "\n",
            "[02:43:03] \u001b[1mTrain data shape: (200000, 120)\u001b[0m\n",
            "\n",
            "[02:44:59] Layer \u001b[1m1\u001b[0m train process start. Time left 3483.39 secs\n",
            "[02:45:09] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
            "[02:50:31] Time limit exceeded after calculating fold 0\n",
            "\n",
            "[02:50:31] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m-0.8717778549643784\u001b[0m\n",
            "[02:50:31] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
            "[02:50:31] Time left 3151.35 secs\n",
            "\n",
            "[02:50:43] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
            "[02:56:45] Time limit exceeded after calculating fold 1\n",
            "\n",
            "[02:56:45] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-0.8598405221537454\u001b[0m\n",
            "[02:56:45] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "[02:56:45] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n"
          ]
        }
      ],
      "source": [
        "oof_pred = automl.fit_predict(\n",
        "    df,\n",
        "    roles = {'target': 'end_cluster'}\n",
        "    , verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ac101d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "ans = automl.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "aa53075e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(ans.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "66d8f2b4",
      "metadata": {
        "id": "66d8f2b4"
      },
      "outputs": [],
      "source": [
        "data = {} #{'id': test.id.to_list()}\n",
        "for cls, prob in zip(category_mapping.keys(), ans):\n",
        "    data[cls] = prob\n",
        "\n",
        "column_mapping = {\n",
        "    0: '{other}',\n",
        "    1: '{}',\n",
        "    2: '{α, β}',\n",
        "    3: '{α, γ}',\n",
        "    4: '{α, δ}',\n",
        "    5: '{α, ε, η}',\n",
        "    6: '{α, ε, θ}',\n",
        "    7: '{α, ε, ψ}',\n",
        "    8: '{α, ε}',\n",
        "    9: '{α, η}',\n",
        "    10: '{α, θ}',\n",
        "    11: '{α, λ}',\n",
        "    12: '{α, μ}',\n",
        "    13: '{α, π}',\n",
        "    14: '{α, ψ}',\n",
        "    15: '{α}',\n",
        "    16: '{λ}'\n",
        "}\n",
        "\n",
        "output = pd.DataFrame(ans.data)\n",
        "output = output.rename(columns=column_mapping)\n",
        "output = output.assign(id=test['id'].tolist())\n",
        "\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
        "output[list(sample_submission_df.columns)].to_csv('ans100.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61ae3af",
      "metadata": {
        "id": "d61ae3af",
        "outputId": "598e3e98-d73a-46fa-8307-fc4fa4396c7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id',\n",
              " '{other}',\n",
              " '{}',\n",
              " '{α, β}',\n",
              " '{α, γ}',\n",
              " '{α, δ}',\n",
              " '{α, ε, η}',\n",
              " '{α, ε, θ}',\n",
              " '{α, ε, ψ}',\n",
              " '{α, ε}',\n",
              " '{α, η}',\n",
              " '{α, θ}',\n",
              " '{α, λ}',\n",
              " '{α, μ}',\n",
              " '{α, π}',\n",
              " '{α, ψ}',\n",
              " '{α}',\n",
              " '{λ}']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
        "list(sample_submission_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a205b27",
      "metadata": {
        "id": "1a205b27",
        "outputId": "c334312f-9581-42d1-f601-9caa0d39654e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e4ead55ff0a49c8ee3cc879f2470f4d9\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "\n",
        "def generate_md5(file_path):\n",
        "    # Open the file in binary mode\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        # Read the content of the file\n",
        "        content = file.read()\n",
        "        # Generate the MD5 hash of the content\n",
        "        md5_hash = hashlib.md5(content).hexdigest()\n",
        "        return md5_hash\n",
        "\n",
        "# Call the function to generate the MD5 hash of a video file\n",
        "# Replace \"video.mp4\" with the path to your video file\n",
        "md5_hash = generate_md5(r\"C:\\Users\\alina\\Videos\\2024-03-12 14-31-10.mkv\")\n",
        "\n",
        "# Print the MD5 hash\n",
        "print(md5_hash)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221f313",
      "metadata": {
        "id": "3221f313"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
