{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b7bef09-9e86-4b19-bbb7-e0ccb7515328",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-14T22:50:38.627643Z",
          "iopub.status.busy": "2024-02-14T22:50:38.627255Z",
          "iopub.status.idle": "2024-02-14T22:50:39.496067Z",
          "shell.execute_reply": "2024-02-14T22:50:39.495215Z",
          "shell.execute_reply.started": "2024-02-14T22:50:38.627616Z"
        },
        "id": "8b7bef09-9e86-4b19-bbb7-e0ccb7515328",
        "tags": []
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0ddb399-18f4-4051-86c0-39f1c623eef5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-15T11:34:39.109032Z",
          "iopub.status.busy": "2024-02-15T11:34:39.108624Z",
          "iopub.status.idle": "2024-02-15T11:34:39.955360Z",
          "shell.execute_reply": "2024-02-15T11:34:39.954584Z",
          "shell.execute_reply.started": "2024-02-15T11:34:39.109007Z"
        },
        "id": "e0ddb399-18f4-4051-86c0-39f1c623eef5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import math\n",
        "import from lightautoml\n",
        "import pickle\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import optuna\n",
        "import catboost as cb\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "WremRMspEMJA",
      "metadata": {
        "id": "WremRMspEMJA"
      },
      "outputs": [],
      "source": [
        "# !pip freeze | grep \"numpy\\|pandas\\|lightgbm\\|scikit-learn\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcf3aae-5417-42a3-9ed2-818b9aef0f2f",
      "metadata": {
        "id": "cbcf3aae-5417-42a3-9ed2-818b9aef0f2f"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "32c8b1b0-f95e-4cb5-bbc1-c0edb76568c1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-15T11:34:44.750723Z",
          "iopub.status.busy": "2024-02-15T11:34:44.750145Z",
          "iopub.status.idle": "2024-02-15T11:34:46.098228Z",
          "shell.execute_reply": "2024-02-15T11:34:46.097494Z",
          "shell.execute_reply.started": "2024-02-15T11:34:44.750694Z"
        },
        "id": "32c8b1b0-f95e-4cb5-bbc1-c0edb76568c1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_parquet(\"train_data.pqt\")\n",
        "# test_df = pd.read_parquet(\"test_df.pqt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06044363",
      "metadata": {
        "id": "06044363"
      },
      "source": [
        "заполянем 5 месяцем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "70f9cc0b",
      "metadata": {
        "id": "70f9cc0b"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_parquet(\"test_data.pqt\")\n",
        "test_df['start_cluster'] = test_df['start_cluster'].fillna(method='ffill')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d48a01",
      "metadata": {
        "id": "69d48a01"
      },
      "source": [
        "вытягиваем в колбасу по id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "a38b5b6b",
      "metadata": {
        "id": "a38b5b6b"
      },
      "outputs": [],
      "source": [
        "grouped_df_first = test_df.groupby('id').first().reset_index()\n",
        "merged_df = pd.merge(test_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
        "\n",
        "grouped_df_second = test_df.groupby('id').nth(1).reset_index()\n",
        "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
        "\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "45e22331",
      "metadata": {
        "id": "45e22331"
      },
      "outputs": [],
      "source": [
        "test_df = merged_df[merged_df['date'] == 'month_6']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "81f271d2",
      "metadata": {
        "id": "81f271d2"
      },
      "outputs": [],
      "source": [
        "grouped_df_first = train_df.groupby('id').first().reset_index()\n",
        "merged_df = pd.merge(train_df, grouped_df_first, on='id', suffixes=('', '_first'))\n",
        "\n",
        "grouped_df_second = train_df.groupby('id').nth(1).reset_index()\n",
        "merged_df = pd.merge(merged_df, grouped_df_second, on='id', suffixes=('', '_second'))\n",
        "\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "99ce934c",
      "metadata": {
        "id": "99ce934c"
      },
      "outputs": [],
      "source": [
        "train_df = merged_df[merged_df['date'] == 'month_3']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22b60ce",
      "metadata": {
        "id": "c22b60ce"
      },
      "source": [
        "удаляем сильную корреляцию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "cbe87424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "cbe87424",
        "outputId": "9f282187-34c0-4fff-8f8b-33dffdc8fe94"
      },
      "outputs": [],
      "source": [
        "corr_matrix = train_df.corr().abs()\n",
        "\n",
        "# Получение верхнего треугольника матрицы корреляции (без диагонали)\n",
        "upper_triangle = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Нахождение колонок, где корреляция больше 0.9\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
        "\n",
        "train_df =train_df.drop(to_drop, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "ec78c3b5",
      "metadata": {
        "id": "ec78c3b5"
      },
      "outputs": [],
      "source": [
        "test_df =test_df.drop(to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "e50a2c22",
      "metadata": {
        "id": "e50a2c22"
      },
      "outputs": [],
      "source": [
        "category_columns = ['start_cluster', 'channel_code', 'city', 'city_type',\n",
        "                    'index_city_code', 'ogrn_month', 'ogrn_year', 'okved', 'segment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "fdf0c11c",
      "metadata": {
        "id": "fdf0c11c"
      },
      "outputs": [],
      "source": [
        "# counts = train_df['city_type'].value_counts()\n",
        "\n",
        "# # Построение гистограммы\n",
        "# plt.bar(counts.index, counts.values)\n",
        "# plt.xlabel('Уникальные значения')\n",
        "# plt.ylabel('Частота')\n",
        "# plt.title('Гистограмма количества встречающихся раз категориальных значений')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dae8244",
      "metadata": {
        "id": "0dae8244"
      },
      "source": [
        "обработка текстов и нан"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "a64e1d8c",
      "metadata": {
        "id": "a64e1d8c",
        "outputId": "503a5eba-5813-4e5d-8f18-db824a2383c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2          channel_code_5\n",
              "5          channel_code_2\n",
              "8         channel_code_12\n",
              "11        channel_code_14\n",
              "14         channel_code_8\n",
              "               ...       \n",
              "599987     channel_code_9\n",
              "599990    channel_code_14\n",
              "599993     channel_code_8\n",
              "599996     channel_code_9\n",
              "599999    channel_code_14\n",
              "Name: channel_code, Length: 200000, dtype: object"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['channel_code']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "c77de816",
      "metadata": {
        "id": "c77de816"
      },
      "outputs": [],
      "source": [
        "# def clever_one_hot(df, col): #переписать в lable encoding?\n",
        "#     top_4 = df[col].value_counts().index[:4]\n",
        "#     df.loc[~df[col].isin(top_4), col] = 'other'\n",
        "#     one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
        "#     df = df.drop(col, axis=1)\n",
        "#     return pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "def feature_prossesing(df):\n",
        "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
        "    df = df.drop('segment', axis=1)\n",
        "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
        "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
        "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
        "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
        "\n",
        "    del df['ogrn_year']\n",
        "    del df['ogrn_month']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    \n",
        "\n",
        "    for i in ['channel_code', 'city_type', 'okved']:\n",
        "        df[i] = label_encoder.fit_transform(df[i])\n",
        "\n",
        "    \n",
        "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
        "    df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
        "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    return df, category_mapping   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "571718d7",
      "metadata": {
        "id": "571718d7"
      },
      "outputs": [],
      "source": [
        "df, category_mapping = feature_prossesing(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "db0e4abe",
      "metadata": {
        "id": "db0e4abe"
      },
      "outputs": [],
      "source": [
        "def clever_one_hot(df, col, df1): #переписать в lable encoding?\n",
        "    top_4 = df1[col].value_counts().index[:4]\n",
        "    df.loc[~df[col].isin(top_4), col] = 'other'\n",
        "    one_hot_encoded = pd.get_dummies(df[col], prefix=col)\n",
        "    df = df.drop(col, axis=1)\n",
        "    return pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "def feature_prossesing(df, df_1):\n",
        "    one_hot_encoded = pd.get_dummies(df['segment'], prefix='seg')\n",
        "    df = df.drop('segment', axis=1)\n",
        "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
        "\n",
        "    df['ogrn_year'] = df['ogrn_year'].fillna('_-1')\n",
        "    df['ogrn_month'] = df['ogrn_month'].fillna('_-1')\n",
        "    df['ogrn_year'] = df['ogrn_year'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['ogrn_month'] = df['ogrn_month'].apply(lambda x: int(x.split('_')[-1]))\n",
        "    df['lasting'] = df['ogrn_year']*12 + df['ogrn_month']\n",
        "    df['lasting'] = df['lasting'].apply(lambda x: x if x>=0 else -1)\n",
        "\n",
        "    del df['ogrn_year']\n",
        "    del df['ogrn_month']\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    \n",
        "\n",
        "    for i in ['channel_code', 'city_type', 'okved']:\n",
        "        df[i] = label_encoder.fit_transform(df[i])\n",
        "\n",
        "    df['start_cluster'] = label_encoder.fit_transform(df['start_cluster'])\n",
        "    #df['end_cluster'] = label_encoder.transform(df['end_cluster'])\n",
        "    category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "    df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    return df, category_mapping   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "fb386861",
      "metadata": {
        "id": "fb386861"
      },
      "outputs": [],
      "source": [
        "test, category_mapping = feature_prossesing(test_df, train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "78538828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78538828",
        "outputId": "864c1f22-c276-4c4b-9d5c-587fac9ff92c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'{other}': 0,\n",
              " '{}': 1,\n",
              " '{α, β}': 2,\n",
              " '{α, γ}': 3,\n",
              " '{α, δ}': 4,\n",
              " '{α, ε, η}': 5,\n",
              " '{α, ε, θ}': 6,\n",
              " '{α, ε, ψ}': 7,\n",
              " '{α, ε}': 8,\n",
              " '{α, η}': 9,\n",
              " '{α, θ}': 10,\n",
              " '{α, λ}': 11,\n",
              " '{α, μ}': 12,\n",
              " '{α, π}': 13,\n",
              " '{α, ψ}': 14,\n",
              " '{α}': 15,\n",
              " '{λ}': 16}"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791378ad",
      "metadata": {
        "id": "791378ad"
      },
      "source": [
        "веса для рок аука которые идут в бустинг"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "1c966fbb",
      "metadata": {
        "id": "1c966fbb"
      },
      "outputs": [],
      "source": [
        "def weighted_roc_auc(y_true, y_pred, labels, weights_dict):\n",
        "    unnorm_weights = np.array([weights_dict[label] for label in labels])\n",
        "    weights = unnorm_weights / unnorm_weights.sum()\n",
        "    classes_roc_auc = roc_auc_score(y_true, y_pred, labels=labels,\n",
        "                                    multi_class=\"ovr\", average=None)\n",
        "    return sum(weights * classes_roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "67791ff1",
      "metadata": {
        "id": "67791ff1"
      },
      "outputs": [],
      "source": [
        "cluster_weights = pd.read_excel(\"cluster_weights.xlsx\").set_index(\"cluster\")\n",
        "weights_dict = cluster_weights[\"unnorm_weight\"].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "b6ea63c8",
      "metadata": {
        "id": "b6ea63c8"
      },
      "outputs": [],
      "source": [
        "weits = [0]*17\n",
        "\n",
        "for k, v in category_mapping.items():\n",
        "    weits[category_mapping[k]] = weights_dict[k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "ec1aaf24",
      "metadata": {
        "id": "ec1aaf24",
        "outputId": "6e303d60-acdd-4660-b03a-f2d50e514b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 1, 3, 3, 3, 1, 1, 3, 2, 2, 1, 3, 2, 1, 3, 2, 2]"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91feaa10",
      "metadata": {
        "id": "91feaa10"
      },
      "source": [
        "треним"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a81d6e35",
      "metadata": {
        "id": "a81d6e35"
      },
      "outputs": [],
      "source": [
        "y = df['end_cluster']\n",
        "X = df.drop(['end_cluster'], axis=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "183b2c43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install lightautoml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c8252ef0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# LightAutoML presets, task and report generation\n",
        "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
        "from lightautoml.tasks import Task\n",
        "from lightautoml.report.report_deco import ReportDeco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a5688eda",
      "metadata": {},
      "outputs": [],
      "source": [
        "task = Task('multiclass')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "5a796892",
      "metadata": {},
      "outputs": [],
      "source": [
        "automl = TabularUtilizedAutoML(\n",
        "    task = Task('multiclass'),\n",
        "    timeout = 3600,\n",
        "    general_params = {'use_algos': [['lgb', 'lgb_tuned']]},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "2d3e0acd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[08:36:02] Start automl \u001b[1mutilizator\u001b[0m with listed constraints:\n",
            "[08:36:02] - time: 3600.00 seconds\n",
            "[08:36:02] - CPU: 4 cores\n",
            "[08:36:02] - memory: 16 GB\n",
            "\n",
            "[08:36:02] \u001b[1mIf one preset completes earlier, next preset configuration will be started\u001b[0m\n",
            "\n",
            "[08:36:02] ==================================================\n",
            "[08:36:02] Start 0 automl preset configuration:\n",
            "[08:36:02] \u001b[1md:\\Anaconda\\lib\\site-packages\\lightautoml\\automl\\presets\\tabular_configs\\conf_0_sel_type_0.yml\u001b[0m, random state: {'reader_params': {'random_state': 42}, 'nn_params': {'random_state': 42}, 'general_params': {'return_all_predictions': False}}\n",
            "[08:36:02] Stdout logging level is INFO.\n",
            "[08:36:02] Task: multiclass\n",
            "\n",
            "[08:36:02] Start automl preset with listed constraints:\n",
            "[08:36:02] - time: 3600.00 seconds\n",
            "[08:36:02] - CPU: 4 cores\n",
            "[08:36:02] - memory: 16 GB\n",
            "\n",
            "[08:36:02] \u001b[1mTrain data shape: (200000, 120)\u001b[0m\n",
            "\n",
            "[08:37:59] Layer \u001b[1m1\u001b[0m train process start. Time left 3483.20 secs\n",
            "[08:38:20] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
            "[08:52:32] Time limit exceeded after calculating fold 2\n",
            "\n",
            "[08:52:33] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-0.8520847690000815\u001b[0m\n",
            "[08:52:33] \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
            "[08:52:33] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
            "[09:00:10] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
            "[09:00:10] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
            "[09:31:38] Fitting \u001b[1mLvl_0_Pipe_0_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m-0.8486530717581346\u001b[0m\n",
            "[09:31:38] \u001b[1mLvl_0_Pipe_0_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
            "[09:31:38] Time left 263.51 secs\n",
            "\n",
            "[09:31:38] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
            "\n",
            "[09:31:38] \u001b[1mLayer 1 training completed.\u001b[0m\n",
            "\n",
            "[09:31:39] Blending: optimization starts with equal weights and score \u001b[1m-0.8412213522348465\u001b[0m\n",
            "[09:31:41] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m-0.8410987582752095\u001b[0m, weights = \u001b[1m[0.5812271  0.41877285]\u001b[0m\n",
            "[09:31:44] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m-0.8410987582752095\u001b[0m, weights = \u001b[1m[0.5812271  0.41877285]\u001b[0m\n",
            "[09:31:44] Blending: no score update. Terminated\n",
            "\n",
            "[09:31:44] \u001b[1mAutoml preset training completed in 3342.58 seconds\u001b[0m\n",
            "\n",
            "[09:31:44] Model description:\n",
            "Final prediction for new objects (level 0) = \n",
            "\t 0.58123 * (3 averaged models Lvl_0_Pipe_0_Mod_0_LightGBM) +\n",
            "\t 0.41877 * (5 averaged models Lvl_0_Pipe_0_Mod_1_Tuned_LightGBM) \n",
            "\n",
            "[09:31:44] ==================================================\n"
          ]
        }
      ],
      "source": [
        "oof_pred = automl.fit_predict(\n",
        "    df,\n",
        "    roles = {'target': 'end_cluster'}\n",
        "    , verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "ac101d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "ans = automl.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "aa53075e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(ans.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d8f2b4",
      "metadata": {
        "id": "66d8f2b4"
      },
      "outputs": [],
      "source": [
        "data = {} #{'id': test.id.to_list()}\n",
        "for cls, prob in zip(category_mapping.keys(), ans):\n",
        "    data[cls] = prob\n",
        "\n",
        "column_mapping = {\n",
        "    0: '{other}',\n",
        "    1: '{}',\n",
        "    2: '{α, β}',\n",
        "    3: '{α, γ}',\n",
        "    4: '{α, δ}',\n",
        "    5: '{α, ε, η}',\n",
        "    6: '{α, ε, θ}',\n",
        "    7: '{α, ε, ψ}',\n",
        "    8: '{α, ε}',\n",
        "    9: '{α, η}',\n",
        "    10: '{α, θ}',\n",
        "    11: '{α, λ}',\n",
        "    12: '{α, μ}',\n",
        "    13: '{α, π}',\n",
        "    14: '{α, ψ}',\n",
        "    15: '{α}',\n",
        "    16: '{λ}'\n",
        "}\n",
        "\n",
        "output = pd.DataFrame(ans.data)\n",
        "output = output.rename(columns=column_mapping)\n",
        "output = output.assign(id=test['id'].tolist())\n",
        "\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
        "output[list(sample_submission_df.columns)].to_csv('ans104.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61ae3af",
      "metadata": {
        "id": "d61ae3af",
        "outputId": "598e3e98-d73a-46fa-8307-fc4fa4396c7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id',\n",
              " '{other}',\n",
              " '{}',\n",
              " '{α, β}',\n",
              " '{α, γ}',\n",
              " '{α, δ}',\n",
              " '{α, ε, η}',\n",
              " '{α, ε, θ}',\n",
              " '{α, ε, ψ}',\n",
              " '{α, ε}',\n",
              " '{α, η}',\n",
              " '{α, θ}',\n",
              " '{α, λ}',\n",
              " '{α, μ}',\n",
              " '{α, π}',\n",
              " '{α, ψ}',\n",
              " '{α}',\n",
              " '{λ}']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
        "list(sample_submission_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a205b27",
      "metadata": {
        "id": "1a205b27",
        "outputId": "c334312f-9581-42d1-f601-9caa0d39654e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e4ead55ff0a49c8ee3cc879f2470f4d9\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "\n",
        "def generate_md5(file_path):\n",
        "    # Open the file in binary mode\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        # Read the content of the file\n",
        "        content = file.read()\n",
        "        # Generate the MD5 hash of the content\n",
        "        md5_hash = hashlib.md5(content).hexdigest()\n",
        "        return md5_hash\n",
        "\n",
        "# Call the function to generate the MD5 hash of a video file\n",
        "# Replace \"video.mp4\" with the path to your video file\n",
        "md5_hash = generate_md5(r\"C:\\Users\\alina\\Videos\\2024-03-12 14-31-10.mkv\")\n",
        "\n",
        "# Print the MD5 hash\n",
        "print(md5_hash)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221f313",
      "metadata": {
        "id": "3221f313"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
